{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ed97ad",
   "metadata": {},
   "source": [
    "# ISLES 2022 UNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q 'monai[all]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "472e4076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from funcs import get_paths,load_metrics, save_metrics, rand_crop\n",
    "\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    Rotated,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    Invertd,\n",
    "    Rotate90d,\n",
    "    RandRotated,\n",
    "    RandShiftIntensityd,\n",
    "    RandGaussianNoised\n",
    ")\n",
    "from monai.handlers.utils import from_engine\n",
    "\n",
    "from UNet3D import UNet\n",
    "from AttUNet import AttUNet\n",
    "from TransUNet import TransUNet\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b882943",
   "metadata": {},
   "source": [
    "# Change Device to Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25a5be71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.cuda.get_device_properties(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e98493",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ebc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = os.path.join(os.getcwd(), 'metrics')\n",
    "\n",
    "saved_files = load_metrics(\"brats_datasplit\")\n",
    "    \n",
    "train_files, val_files, test_files = saved_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1170c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = os.path.join(os.getcwd(), 'metrics')\n",
    "\n",
    "image_paths = get_paths('*T1w*')\n",
    "label_paths = get_paths('*mask*')\n",
    "\n",
    "# To ensure I am not pulling in the training set with no mask/labels\n",
    "assert(len(image_paths) == len(label_paths))\n",
    "data_length = len(image_paths)\n",
    "\n",
    "data_dicts = [\n",
    "    {\"image\": image_name, \"label\": label_name}\n",
    "    for image_name, label_name in zip(image_paths, label_paths)\n",
    "]\n",
    "\n",
    "# Because, why not? \n",
    "assert(len(data_dicts) == data_length)\n",
    "\n",
    "num_of_cases = len(data_dicts)\n",
    "train_size = ceil((num_of_cases / 100) * 70)\n",
    "validation_size = floor((num_of_cases / 100) * 15)\n",
    "test_size = floor((num_of_cases / 100) * 15)\n",
    "\n",
    "assert(train_size+validation_size+test_size == len(data_dicts))\n",
    "\n",
    "random.shuffle(data_dicts)\n",
    "\n",
    "train_files = data_dicts[:train_size]\n",
    "val_files = data_dicts[train_size:train_size+validation_size]\n",
    "test_files = data_dicts[train_size+validation_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c471ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_files))\n",
    "print(len(val_files))\n",
    "print(len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0492e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # store this specific datasplit for future training\n",
    "# save_metrics('datasplit', (train_files, val_files, test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f129e8",
   "metadata": {},
   "source": [
    "# Transforms using Monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902aad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]), # Load image file or files from provided path based on reader.\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]), #adds a channel dimension if the data doesn't have one ... torch.Size([1, ...]) = torch.Size([1, 1, ...\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"LPS\"),\n",
    "        Rotate90d(keys=[\"image\", \"label\"], k=1, spatial_axes=(0,2)), # rotate data so it looks like it should do? ... doesn't feel right when viewing otherwise\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"], a_min=0.0, a_max=302.0,\n",
    "            b_min=0.0, b_max=1.0, clip=True,\n",
    "        ),\n",
    "        RandRotated(keys=[\"image\", \"label\"], prob=0.2, range_x=0.3),\n",
    "        RandRotated(keys=[\"image\", \"label\"], prob=0.2, range_y=0.3),\n",
    "        RandRotated(keys=[\"image\", \"label\"], prob=0.2, range_z=0.3),\n",
    "        RandShiftIntensityd(\n",
    "            keys=[\"image\"],\n",
    "            offsets=0.10,\n",
    "            prob=0.50,\n",
    "        ),\n",
    "        RandGaussianNoised(keys=[\"image\"]),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]) # converts the data to a pytorch tensor\n",
    "    ]\n",
    ")\n",
    "\n",
    "        \n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]), # Load image file or files from provided path based on reader.\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]), #adds a channel dimension if the data doesn't have one ... torch.Size([1, ...]) = torch.Size([1, 1, ...\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"LPS\"),\n",
    "        Rotate90d(keys=[\"image\", \"label\"], k=1, spatial_axes=(0,2)), # rotate data so it looks like it should do? ... doesn't feel right when viewing otherwise\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"], a_min=0.0, a_max=302.0,\n",
    "            b_min=0.0, b_max=1.0, clip=True,\n",
    "        ),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]) # converts the data to a pytorch tensor\n",
    "    ]\n",
    ")\n",
    "        \n",
    "cropper = RandCropByPosNegLabeld(\n",
    "    keys=[\"image\", \"label\"],\n",
    "    label_key=\"label\",\n",
    "    spatial_size=(96, 96, 96),   # provides size of each image within the batch\n",
    "    pos=1,      # pos / (pos + neg) = ratio of postivie and negative samples picked... \n",
    "    neg=1,      # with pos = neg = 1, ratio = 0.5 so it picks equal pos (stoke) and neg (no stroke) for sample.\n",
    "    num_samples=4,   # number of smaller volumes to create from the original volume\n",
    "    image_key=\"image\",\n",
    "    image_threshold=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type == 'cuda':\n",
    "    train_ds = CacheDataset(\n",
    "        data=train_files, \n",
    "        transform=train_transforms,\n",
    "        cache_rate=1.0, \n",
    "        num_workers=4\n",
    "    )\n",
    "    val_ds = CacheDataset(\n",
    "        data=val_files, \n",
    "        transform=val_transforms, \n",
    "        cache_rate=1.0, \n",
    "        num_workers=4\n",
    "    )\n",
    "else:\n",
    "    train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "    val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "\n",
    "# 4 batch size in DataLoader and 4 samples per scan from RandCropByPosNegLabeld creates an actual batch size of 16 ... data has shape (16, 1, 223, 197, 189)\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c5585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = UNet().to(device)\n",
    "\n",
    "# model = AttUNet().to(device)\n",
    "\n",
    "# model = TransUNet().to(device)\n",
    "\n",
    "model = SwinUNETR(img_size=(96,96,96), in_channels=1, out_channels=2, feature_size=48).to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed7f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a632dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e14db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 300\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "val_epoch_loss_values = []\n",
    "train_metric_values = []\n",
    "metric_values = []\n",
    "post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=2)])\n",
    "post_label = Compose([EnsureType(), AsDiscrete(to_onehot=2)])\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            step += 1\n",
    "            inputs, labels = (\n",
    "                batch_data[\"image\"],\n",
    "                batch_data[\"label\"],\n",
    "            )\n",
    "            inputs, labels = rand_crop(inputs, labels, cropper)\n",
    "            outputs = model(inputs.to(device))\n",
    "            loss = loss_function(outputs, labels.to(device))\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "    step = 0\n",
    "    val_epoch_loss = 0\n",
    "    for batch_data in val_loader:\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"],\n",
    "            batch_data[\"label\"],\n",
    "        )\n",
    "        inputs, labels = rand_crop(inputs, labels, cropper)\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = loss_function(outputs, labels.to(device))\n",
    "        val_epoch_loss += loss.item()\n",
    "    val_epoch_loss /= step\n",
    "    val_epoch_loss_values.append(val_epoch_loss)\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                roi_size = (96, 96, 96)\n",
    "                sw_batch_size = 4\n",
    "                val_outputs = sliding_window_inference(\n",
    "                    val_inputs, roi_size, sw_batch_size, model)\n",
    "                val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                # compute metric for current iteration\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "            # aggregate the final mean dice result\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            # reset the status for next validation round\n",
    "            dice_metric.reset()\n",
    "            metric_values.append(metric)\n",
    "            \n",
    "            for train_data in train_loader:\n",
    "                train_inputs, train_labels = (\n",
    "                    train_data[\"image\"].to(device),\n",
    "                    train_data[\"label\"].to(device),\n",
    "                )\n",
    "                roi_size = (96, 96, 96)\n",
    "                sw_batch_size = 4\n",
    "                train_outputs = sliding_window_inference(\n",
    "                    train_inputs, roi_size, sw_batch_size, model)\n",
    "                train_outputs = [post_pred(i) for i in decollate_batch(train_outputs)]\n",
    "                train_labels = [post_label(i) for i in decollate_batch(train_labels)]\n",
    "                dice_metric(y_pred=train_outputs, y=train_labels)\n",
    "            train_metric = dice_metric.aggregate().item()\n",
    "            dice_metric.reset()        \n",
    "            train_metric_values.append(train_metric)\n",
    "            \n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(\n",
    "                    training_dir, \"best_metric_model.pth\"))\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f} \"\n",
    "                f\"at epoch: {best_metric_epoch}\"\n",
    "            )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e503fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"train completed, best_metric: {best_metric:.4f} \"\n",
    "    f\"at epoch: {best_metric_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf67063",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metrics('unet-values', (epoch_loss_values, val_epoch_loss_values, train_metric_values, metric_values, best_metric, best_metric_epoch))\n",
    "torch.save({\n",
    "            'epoch': max_epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, \"unet_model_and_optim\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:isles2022] *",
   "language": "python",
   "name": "conda-env-isles2022-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
